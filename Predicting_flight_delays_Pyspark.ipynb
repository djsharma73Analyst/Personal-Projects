{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T08:50:52.089008Z",
          "iopub.execute_input": "2021-12-25T08:50:52.089314Z",
          "shell.execute_reply.started": "2021-12-25T08:50:52.089284Z",
          "shell.execute_reply": "2021-12-25T08:51:36.644858Z",
          "iopub.status.idle": "2021-12-25T08:51:36.6458Z"
        },
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "1e17b81d-b480-4fca-ab07-a53e080e7733",
          "inputWidgets": {},
          "title": ""
        },
        "trusted": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiJodefmItCj",
        "outputId": "ce436633-b2c6-41a0-8548-eaae47cfc797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 30 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 51.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=17ffb9cdbea86ac85dcdfa466fc500072262e882c8da2b201941f23e0c748e30\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "4c5215eb-e52d-476e-ad2e-f8d65fa0fefa",
          "inputWidgets": {},
          "title": ""
        },
        "id": "5g1m_pyNItCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing neccessary packages and libraries**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "b60e5d1b-99bc-40af-a6b3-6ca4456666de",
          "inputWidgets": {},
          "title": ""
        },
        "id": "skMmnC02ItCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import os\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "from pyspark.sql.functions import round\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:47:47.708159Z",
          "iopub.execute_input": "2021-12-25T10:47:47.708527Z",
          "shell.execute_reply.started": "2021-12-25T10:47:47.70848Z",
          "shell.execute_reply": "2021-12-25T10:47:47.714122Z",
          "iopub.status.idle": "2021-12-25T10:47:47.714924Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "979aa108-082e-4b4b-a744-98604f94d6e8",
          "inputWidgets": {},
          "title": ""
        },
        "id": "h_2kAtDJItCm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a local Spark cluster using all available cores, which will be accessible via a SparkSession object."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "fdffaff4-097a-4d38-82c6-3fa430c47577",
          "inputWidgets": {},
          "title": ""
        },
        "id": "sqDj9zhPItCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('myproj').getOrCreate()\n",
        "data = spark.read.csv('flights-larger.csv',inferSchema=True,header=True)\n",
        "data.printSchema()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:48:08.367554Z",
          "iopub.execute_input": "2021-12-25T10:48:08.368303Z",
          "shell.execute_reply.started": "2021-12-25T10:48:08.368259Z",
          "shell.execute_reply": "2021-12-25T10:48:08.495519Z",
          "iopub.status.idle": "2021-12-25T10:48:08.496122Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "3a4f28ff-e594-4b50-a898-73fe123b29cd",
          "inputWidgets": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "iaDUgQHOItCn",
        "outputId": "789daadb-1bb1-494c-a93f-3999f4e74eaa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c3b2cbf16620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'myproj'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'flights-larger.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/flights-larger.csv"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJ1f_1DBhZdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights_df = spark.read.csv('flights-larger.csv',inferSchema=True,header=True)\n",
        "for column in flights_df.columns:\n",
        "    if flights_df.schema[column].dataType == pyspark.sql.types.StringType():\n",
        "        flights_df = flights_df.filter(flights_df[column] != \"NA\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:48:13.911096Z",
          "iopub.execute_input": "2021-12-25T10:48:13.911371Z",
          "shell.execute_reply.started": "2021-12-25T10:48:13.911344Z",
          "shell.execute_reply": "2021-12-25T10:48:14.962684Z",
          "iopub.status.idle": "2021-12-25T10:48:14.963553Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "b8acf72a-b886-481c-91e0-bbbda6159b5c",
          "inputWidgets": {},
          "title": ""
        },
        "id": "OHzHPW8_ItCn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "KffF2O2hJjax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of records\n",
        "print(\"The data contain %d records.\" % flights_df.count())\n",
        "# View the first five records\n",
        "flights_df.show(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:48:20.460467Z",
          "iopub.execute_input": "2021-12-25T10:48:20.460757Z",
          "shell.execute_reply.started": "2021-12-25T10:48:20.460728Z",
          "shell.execute_reply": "2021-12-25T10:48:20.827024Z",
          "iopub.status.idle": "2021-12-25T10:48:20.828007Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "dd094a6a-191a-4505-951b-04e599a4686c",
          "inputWidgets": {},
          "title": ""
        },
        "id": "c5xQ7U8gItCn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check column data types\n",
        "print(flights_df.dtypes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:48:23.443378Z",
          "iopub.execute_input": "2021-12-25T10:48:23.44366Z",
          "shell.execute_reply.started": "2021-12-25T10:48:23.443626Z",
          "shell.execute_reply": "2021-12-25T10:48:23.459448Z",
          "iopub.status.idle": "2021-12-25T10:48:23.460038Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "c9385a54-e35f-41e7-84bc-85d03cbeba30",
          "inputWidgets": {},
          "title": ""
        },
        "id": "udEzHIR3ItCo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the 'flight' column\n",
        "flights_df =  flights_df.drop('flight')\n",
        "\n",
        "# Remove records with missing 'delay' values\n",
        "#flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
        "\n",
        "# Remove records with missing values \n",
        "flights_df = flights_df.dropna()\n",
        "print(flights_df.count())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:48:27.138422Z",
          "iopub.execute_input": "2021-12-25T10:48:27.139143Z",
          "shell.execute_reply.started": "2021-12-25T10:48:27.13911Z",
          "shell.execute_reply": "2021-12-25T10:48:27.68939Z",
          "iopub.status.idle": "2021-12-25T10:48:27.69016Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "46e37048-ae55-48de-982b-71c061c14ffa",
          "inputWidgets": {},
          "title": ""
        },
        "id": "Zql6mLbTItCo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "f3Y8jfI-uqB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert columns 'mile' to 'km' and then drop it\n",
        "flights_km = flights_df.withColumn('km', round(flights_df.mile * 1.60934, 0)) \\\n",
        "                    .drop('mile')\n",
        "\n",
        "# Create 'label' column indicating whether a flight is delayed or not\n",
        "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
        "\n",
        "# Check first five records\n",
        "flights_km.show(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:48:34.054347Z",
          "iopub.execute_input": "2021-12-25T10:48:34.054856Z",
          "shell.execute_reply.started": "2021-12-25T10:48:34.054818Z",
          "shell.execute_reply": "2021-12-25T10:48:34.178917Z",
          "iopub.status.idle": "2021-12-25T10:48:34.179804Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "21e4ae97-afd4-4ec1-878d-ec8545461930",
          "inputWidgets": {},
          "title": ""
        },
        "id": "1Wz6pma6ItCo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an indexer, which identifies categories and then creates a new column with numeric index values\n",
        "flights_indexed = StringIndexer(inputCol='carrier', outputCol='carrier_idx').fit(flights_km).transform(flights_km)\n",
        "\n",
        "# Repeat the process for org column\n",
        "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
        "flights_indexed.show(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:48:37.647566Z",
          "iopub.execute_input": "2021-12-25T10:48:37.647877Z",
          "shell.execute_reply.started": "2021-12-25T10:48:37.647846Z",
          "shell.execute_reply": "2021-12-25T10:48:39.566144Z",
          "iopub.status.idle": "2021-12-25T10:48:39.566952Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "64a3c484-aa05-44d2-9079-afaf2d9f49e8",
          "inputWidgets": {},
          "title": ""
        },
        "id": "bhF_SyylItCp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an assembler object\n",
        "assembler = VectorAssembler(inputCols=['mon', 'dom', 'dow',\n",
        "'carrier_idx', 'org_idx', 'km', 'depart', 'duration'], outputCol='features')\n",
        "# Consolidate predictor columns\n",
        "flights_assembled = assembler.transform(flights_indexed)\n",
        "# Check the resulting column\n",
        "flights_assembled.select('features', 'delay').show(5, truncate=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:48:42.159513Z",
          "iopub.execute_input": "2021-12-25T10:48:42.159843Z",
          "shell.execute_reply.started": "2021-12-25T10:48:42.15981Z",
          "shell.execute_reply": "2021-12-25T10:48:42.345286Z",
          "iopub.status.idle": "2021-12-25T10:48:42.346226Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "8e98a3c9-629e-48e3-afce-4a1a74947e2c",
          "inputWidgets": {},
          "title": ""
        },
        "id": "TX09Av7bItCp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Models**"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "0051f348-5df9-4453-baec-4818238cb7e1",
          "inputWidgets": {},
          "title": ""
        },
        "id": "rqV8JkGDItCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and testing sets in a 80:20 ratio\n",
        "flights_train, flights_test = flights_assembled.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Check that training set has around 80% of records\n",
        "training_ratio = flights_train.count() / flights_assembled.count()\n",
        "print(training_ratio)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:49:03.03973Z",
          "iopub.execute_input": "2021-12-25T10:49:03.04006Z",
          "shell.execute_reply.started": "2021-12-25T10:49:03.040007Z",
          "shell.execute_reply": "2021-12-25T10:49:05.935504Z",
          "iopub.status.idle": "2021-12-25T10:49:05.936241Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "033c1409-725f-4f10-a574-ef93dd4ce9b0",
          "inputWidgets": {},
          "title": ""
        },
        "id": "3byDcil4ItCp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DT classifier object and fit to the training data\n",
        "tree = DecisionTreeClassifier()\n",
        "tree_model = tree.fit(flights_train)\n",
        "# Create predictions on test data\n",
        "prediction = tree_model.transform(flights_test)\n",
        "prediction.select('label', 'prediction', 'probability').show(5, False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:49:09.060493Z",
          "iopub.execute_input": "2021-12-25T10:49:09.060816Z",
          "shell.execute_reply.started": "2021-12-25T10:49:09.060781Z",
          "shell.execute_reply": "2021-12-25T10:49:19.888508Z",
          "iopub.status.idle": "2021-12-25T10:49:19.889529Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "fee78052-b9c1-4a30-b8f8-ec7b45d39b21",
          "inputWidgets": {},
          "title": ""
        },
        "id": "PCKvA8K5ItCq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix\n",
        "prediction.groupBy('label', 'prediction').count().show()\n",
        "\n",
        "# Calculate the elements of the confusion matrix\n",
        "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
        "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
        "\n",
        "# Accuracy measures the proportion of correct predictions\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:49:24.367557Z",
          "iopub.execute_input": "2021-12-25T10:49:24.367905Z",
          "shell.execute_reply.started": "2021-12-25T10:49:24.367857Z",
          "shell.execute_reply": "2021-12-25T10:49:33.157336Z",
          "iopub.status.idle": "2021-12-25T10:49:33.158105Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "52bdcf77-6085-4600-9b01-437a7feb6c4e",
          "inputWidgets": {},
          "title": ""
        },
        "id": "TRk_M22RItCq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is decent but not a good one. We have a lot of false predictions."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "b0e068d3-1d31-4787-b973-e7b4f168a52e",
          "inputWidgets": {},
          "title": ""
        },
        "id": "NvXRjcQgItCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a classifier object and train on training data\n",
        "logistic = LogisticRegression().fit(flights_train)\n",
        "# Create predictions for the testing data and show confusion matrix\n",
        "prediction = logistic.transform(flights_test)\n",
        "prediction.groupBy('label', 'prediction').count().show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:49:42.040961Z",
          "iopub.execute_input": "2021-12-25T10:49:42.041244Z",
          "shell.execute_reply.started": "2021-12-25T10:49:42.041216Z",
          "shell.execute_reply": "2021-12-25T10:49:48.426914Z",
          "iopub.status.idle": "2021-12-25T10:49:48.427821Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "e9342e69-1902-4ffd-aa72-e333e78d49f8",
          "inputWidgets": {},
          "title": ""
        },
        "id": "-_r-VdW0ItCr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate precision and recall\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
        "\n",
        "# Find weighted precision\n",
        "multi_evaluator = MulticlassClassificationEvaluator()\n",
        "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "\n",
        "# Find AUC\n",
        "binary_evaluator = BinaryClassificationEvaluator()\n",
        "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:49:55.448622Z",
          "iopub.execute_input": "2021-12-25T10:49:55.449534Z",
          "shell.execute_reply.started": "2021-12-25T10:49:55.449495Z",
          "shell.execute_reply": "2021-12-25T10:50:00.240101Z",
          "iopub.status.idle": "2021-12-25T10:50:00.24078Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "6caddad7-6e81-46f5-b0c0-f08ecb38a2b5",
          "inputWidgets": {},
          "title": ""
        },
        "id": "EAtoTrobItCr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wlf6R6Q4s4bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Close spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-25T10:50:24.620539Z",
          "iopub.execute_input": "2021-12-25T10:50:24.620855Z",
          "shell.execute_reply.started": "2021-12-25T10:50:24.620823Z",
          "shell.execute_reply": "2021-12-25T10:50:25.145336Z",
          "iopub.status.idle": "2021-12-25T10:50:25.145992Z"
        },
        "trusted": true,
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "7f8ad6fa-f5d8-4a42-b9b1-50eb2e5f3b17",
          "inputWidgets": {},
          "title": ""
        },
        "id": "DDg-z6qxItCr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "showTitle": false,
          "cellMetadata": {},
          "nuid": "52c3d92b-105c-4042-8c3b-e4fc36df8c8a",
          "inputWidgets": {},
          "title": ""
        },
        "id": "Hc57U2UNItCr"
      },
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.6.4",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Predicting flight delays - Pyspark",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 1557730265585758
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}